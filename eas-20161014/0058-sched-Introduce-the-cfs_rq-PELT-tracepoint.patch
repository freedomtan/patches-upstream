From ce7106cefa2cbf1ab754328b1c4dee0c57ae6db2 Mon Sep 17 00:00:00 2001
From: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date: Mon, 9 Nov 2015 12:07:27 +0000
Subject: [PATCH 58/61] sched: Introduce the 'cfs_rq PELT' tracepoint

To let this tracepoint work on all configurations, the following special
handling is necessary for non-existent key=value
pairs:

id = -1 : In case of !CONFIG_FAIR_GROUP_SCHED the task group css id is
          set to -1.

Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
---
 include/trace/events/sched.h | 43 +++++++++++++++++++++++++++++++++++++++++++
 kernel/sched/fair.c          |  7 +++++++
 2 files changed, 50 insertions(+)

diff --git a/include/trace/events/sched.h b/include/trace/events/sched.h
index 9b90c57..c8c2a40 100644
--- a/include/trace/events/sched.h
+++ b/include/trace/events/sched.h
@@ -562,6 +562,49 @@ TRACE_EVENT(sched_wake_idle_without_ipi,
 
 	TP_printk("cpu=%d", __entry->cpu)
 );
+
+/*
+ * Tracepoint for cfs_rq Per Entity Load Tracking (PELT).
+ */
+TRACE_EVENT(sched_pelt_cfs_rq,
+
+	TP_PROTO(struct cfs_rq *cfs_rq),
+
+	TP_ARGS(cfs_rq),
+
+	TP_STRUCT__entry(
+		__field( int,		cpu			)
+		__field( int,		id			)
+		__field( unsigned long,	load_avg		)
+		__field( unsigned long,	util_avg		)
+		__field( u64,		load_sum		)
+		__field( u32,		util_sum		)
+		__field( u32,		period_contrib		)
+		__field( u64,		last_update_time	)
+	),
+
+	TP_fast_assign(
+#ifdef CONFIG_FAIR_GROUP_SCHED
+		__entry->cpu			= cfs_rq->rq->cpu;
+		__entry->id			= cfs_rq->tg->css.id;
+#else
+		__entry->cpu			= (container_of(cfs_rq, struct rq, cfs))->cpu;
+		__entry->id			= -1;
+#endif
+		__entry->load_avg		= cfs_rq->avg.load_avg;
+		__entry->util_avg		= cfs_rq->avg.util_avg;
+		__entry->load_sum		= cfs_rq->avg.load_sum;
+		__entry->util_sum		= cfs_rq->avg.util_sum;
+		__entry->period_contrib		= cfs_rq->avg.period_contrib;
+		__entry->last_update_time	= cfs_rq->avg.last_update_time;
+	),
+
+	TP_printk("cpu=%d tg_css_id=%d load_avg=%lu util_avg=%lu"
+		  " load_sum=%llu util_sum=%u period_contrib=%u last_update_time=%llu",
+		  __entry->cpu, __entry->id, __entry->load_avg,
+		  __entry->util_avg, __entry->load_sum, __entry->util_sum,
+		  __entry->period_contrib, __entry->last_update_time)
+);
 #endif /* _TRACE_SCHED_H */
 
 /* This part must be outside protection */
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 62f80d7..619644e 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -2882,6 +2882,9 @@ __update_load_avg(u64 now, int cpu, struct sched_avg *sa,
 		sa->util_avg = sa->util_sum / LOAD_AVG_MAX;
 	}
 
+	if (cfs_rq)
+		trace_sched_pelt_cfs_rq(cfs_rq);
+
 	return decayed;
 }
 
@@ -3116,6 +3119,8 @@ static void attach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *s
 	cfs_rq->avg.util_sum += se->avg.util_sum;
 
 	cfs_rq_util_change(cfs_rq);
+
+	trace_sched_pelt_cfs_rq(cfs_rq);
 }
 
 /**
@@ -3138,6 +3143,8 @@ static void detach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *s
 	sub_positive(&cfs_rq->avg.util_sum, se->avg.util_sum);
 
 	cfs_rq_util_change(cfs_rq);
+
+	trace_sched_pelt_cfs_rq(cfs_rq);
 }
 
 /* Add the load generated by se into cfs_rq's load average */
-- 
2.7.4

