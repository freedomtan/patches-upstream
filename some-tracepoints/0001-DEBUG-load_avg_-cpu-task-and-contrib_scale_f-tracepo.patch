From 0f0ede0c40bbbd111becf7978d129ecd52a16cfc Mon Sep 17 00:00:00 2001
From: Juri Lelli <juri.lelli@arm.com>
Date: Thu, 12 Feb 2015 17:55:47 +0000
Subject: [PATCH 1/3] DEBUG: load_avg_{cpu,task} and contrib_scale_f
 tracepoints

adapted for 4.3-rc1 (PELT rewrite problem)

Change-Id: I923425a231cbbc5dfac86fc3e2a90459761bd2ea
Signed-off-by: Juri Lelli <juri.lelli@arm.com>
Signed-off-by: Koan-Sin Tan <freedom.tan@linaro.org>
---
 include/trace/events/sched.h | 91 ++++++++++++++++++++++++++++++++++++++++++++
 kernel/sched/fair.c          |  7 ++++
 2 files changed, 98 insertions(+)

diff --git a/include/trace/events/sched.h b/include/trace/events/sched.h
index 9b90c57..086d716 100644
--- a/include/trace/events/sched.h
+++ b/include/trace/events/sched.h
@@ -562,6 +562,97 @@ TRACE_EVENT(sched_wake_idle_without_ipi,
 
 	TP_printk("cpu=%d", __entry->cpu)
 );
+
+TRACE_EVENT(sched_contrib_scale_f,
+
+	TP_PROTO(int cpu, unsigned long freq_scale_factor,
+		 unsigned long cpu_scale_factor),
+
+	TP_ARGS(cpu, freq_scale_factor, cpu_scale_factor),
+
+	TP_STRUCT__entry(
+		__field(int, cpu)
+		__field(unsigned long, freq_scale_factor)
+		__field(unsigned long, cpu_scale_factor)
+	),
+
+	TP_fast_assign(
+		__entry->cpu = cpu;
+		__entry->freq_scale_factor = freq_scale_factor;
+		__entry->cpu_scale_factor = cpu_scale_factor;
+	),
+
+	TP_printk("cpu=%d freq_scale_factor=%lu cpu_scale_factor=%lu",
+		  __entry->cpu, __entry->freq_scale_factor,
+		  __entry->cpu_scale_factor)
+);
+
+/*
+ * Tracepoint for accounting sched averages for tasks.
+ */
+TRACE_EVENT(sched_load_avg_task,
+
+	TP_PROTO(struct task_struct *tsk, struct sched_avg *avg),
+
+	TP_ARGS(tsk, avg),
+
+	TP_STRUCT__entry(
+		__array( char,	comm,	TASK_COMM_LEN		)
+		__field( pid_t,	pid				)
+		__field( unsigned long,	last_update_time	)
+		__field( unsigned long,	load_sum		)
+		__field( unsigned int,	util_sum		)
+		__field( unsigned int,	period_contrib		)
+		__field( unsigned long,	load_avg		)
+		__field( unsigned long,	util_avg		)
+	),
+
+	TP_fast_assign(
+		memcpy(__entry->comm, tsk->comm, TASK_COMM_LEN);
+		__entry->pid			= tsk->pid;
+		__entry->last_update_time	= avg->last_update_time;
+		__entry->load_sum		= avg->load_sum;
+		__entry->util_sum		= avg->util_sum;
+		__entry->period_contrib		= avg->period_contrib;
+		__entry->load_avg		= avg->load_avg;
+		__entry->util_avg		= avg->util_avg;
+	),
+
+	TP_printk("comm=%s pid=%d last_update=%lu load_sum=%lu util_sum=%u "
+		  "period_contrib=%u load_avg=%lu util_avg=%lu",
+		  __entry->comm, __entry->pid, __entry->last_update_time,
+		  __entry->load_sum, __entry->util_sum,
+		  __entry->period_contrib,
+		  __entry->load_avg, __entry->util_avg)
+);
+
+/*
+ * Tracepoint for accounting sched averages for cpus.
+ */
+TRACE_EVENT(sched_load_avg_cpu,
+
+	TP_PROTO(int cpu, struct cfs_rq *cfs_rq),
+
+	TP_ARGS(cpu, cfs_rq),
+
+	TP_STRUCT__entry(
+		__field( int,	cpu				)
+		__field( unsigned long,	load			)
+		__field( unsigned long,	load_avg		)
+		__field( unsigned long,	util_avg		)
+	),
+
+	TP_fast_assign(
+		__entry->cpu			= cpu;
+		__entry->load			= cfs_rq->runnable_load_avg;
+		__entry->load_avg		= cfs_rq->avg.load_avg;
+		__entry->util_avg		= cfs_rq->avg.util_avg;
+	),
+
+	TP_printk("cpu=%d load=%lu load_avg=%lu util_avg=%lu",
+		__entry->cpu, __entry->load,
+		__entry->load_avg, __entry->util_avg)
+);
 #endif /* _TRACE_SCHED_H */
 
 /* This part must be outside protection */
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index db0d536..934a641 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -2588,6 +2588,8 @@ __update_load_avg(u64 now, int cpu, struct sched_avg *sa,
 	scale_freq = arch_scale_freq_capacity(NULL, cpu);
 	scale_cpu = arch_scale_cpu_capacity(NULL, cpu);
 
+	trace_sched_contrib_scale_f(cpu, scale_freq, scale_cpu);
+
 	/* delta_w is the amount already accumulated against our next period */
 	delta_w = sa->period_contrib;
 	if (delta + delta_w >= 1024) {
@@ -2730,6 +2732,11 @@ static inline void update_load_avg(struct sched_entity *se, int update_tg)
 
 	if (update_cfs_rq_load_avg(now, cfs_rq) && update_tg)
 		update_tg_load_avg(cfs_rq, 0);
+
+	if (entity_is_task(se))
+		trace_sched_load_avg_task(task_of(se), &se->avg);
+
+	trace_sched_load_avg_cpu(cpu, cfs_rq);
 }
 
 static void attach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)
-- 
1.9.1

